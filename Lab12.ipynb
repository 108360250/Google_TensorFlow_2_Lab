{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab12.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNWFCUW+MhvUYC0Un+YxqUy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tensorflow-addons"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6rpMgVWqn5oj","executionInfo":{"status":"ok","timestamp":1642257306715,"user_tz":-480,"elapsed":3964,"user":{"displayName":"北科大-林樂基","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15721416263770931233"}},"outputId":"72417d36-5a74-462e-c192-fa17c790808c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 10.4 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.15.0\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/KUASWoodyLIN/TF2-Yolo3.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIb8hhDApEXp","executionInfo":{"status":"ok","timestamp":1642257308169,"user_tz":-480,"elapsed":1456,"user":{"displayName":"北科大-林樂基","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15721416263770931233"}},"outputId":"94162872-0bf4-41d4-ce18-4d1af962dd30"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'TF2-Yolo3'...\n","remote: Enumerating objects: 292, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 292 (delta 0), reused 4 (delta 0), pack-reused 288\u001b[K\n","Receiving objects: 100% (292/292), 9.55 MiB | 23.18 MiB/s, done.\n","Resolving deltas: 100% (143/143), done.\n"]}]},{"cell_type":"code","source":["%cd /content/TF2-Yolo3/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bSoggwR4M-it","executionInfo":{"status":"ok","timestamp":1642258230459,"user_tz":-480,"elapsed":330,"user":{"displayName":"北科大-林樂基","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15721416263770931233"}},"outputId":"591700b2-8a80-46a1-8f21-32255ae337e8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/TF2-Yolo3\n"]}]},{"cell_type":"code","source":["!mkdir model_data"],"metadata":{"id":"O_P2u3ZpKhxe","executionInfo":{"status":"ok","timestamp":1642257572978,"user_tz":-480,"elapsed":355,"user":{"displayName":"北科大-林樂基","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15721416263770931233"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!wget https://pjreddie.com/media/files/yolov3.weights -o model_data/yolov3.weights"],"metadata":{"id":"1eacMnBGprqU","executionInfo":{"status":"ok","timestamp":1642257713465,"user_tz":-480,"elapsed":15794,"user":{"displayName":"北科大-林樂基","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15721416263770931233"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!python convert.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4GwwtDUrmlv","executionInfo":{"status":"ok","timestamp":1642258267734,"user_tz":-480,"elapsed":7492,"user":{"displayName":"北科大-林樂基","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15721416263770931233"}},"outputId":"cf01e10e-7cb0-477d-ed73-4a209e96d4be"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-01-15 14:51:03.545474: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization/gamma:0' shape=(32,) dtype=float32>\n","  <tf.Variable 'batch_normalization/beta:0' shape=(32,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_1), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_1/gamma:0' shape=(64,) dtype=float32>\n","  <tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_2/kernel:0' shape=(1, 1, 64, 32) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_2), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_2/gamma:0' shape=(32,) dtype=float32>\n","  <tf.Variable 'batch_normalization_2/beta:0' shape=(32,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_3), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_3/gamma:0' shape=(64,) dtype=float32>\n","  <tf.Variable 'batch_normalization_3/beta:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_4), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_4/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_4/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_5/kernel:0' shape=(1, 1, 128, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_5), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_5/gamma:0' shape=(64,) dtype=float32>\n","  <tf.Variable 'batch_normalization_5/beta:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_6), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_6/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_6/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_7/kernel:0' shape=(1, 1, 128, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_7), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_7/gamma:0' shape=(64,) dtype=float32>\n","  <tf.Variable 'batch_normalization_7/beta:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_8/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_8), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_8/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_8/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_9/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_9), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_9/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_9/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_10/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_10), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_10/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_10/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_11/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_11), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_11/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_11/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_12/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_12), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_12/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_12/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_13/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_13), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_13/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_13/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_14/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_14), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_14/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_14/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_15/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_15), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_15/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_15/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_16), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_16/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_16), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_16/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_16/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_17), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_17/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_17), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_17/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_17/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_18), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_18/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_18), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_18/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_18/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_19), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_19/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_19), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_19/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_19/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_20), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_20/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_20), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_20/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_20/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_21), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_21/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_21), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_21/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_21/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_22), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_22/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_22), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_22/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_22/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_23), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_23/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_23), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_23/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_23/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_24), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_24/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_24), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_24/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_24/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_25), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_25), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_25/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_25/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_26), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_26), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_26/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_26/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_27), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_27/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_27), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_27/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_27/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_28), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_28/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_28), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_28/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_28/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_29), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_29/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_29), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_29/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_29/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_30), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_30/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_30), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_30/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_30/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_31), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_31/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_31), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_31/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_31/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_32), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_32/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_32), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_32/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_32/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_33), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_33/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_33), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_33/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_33/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_34), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_34/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_34), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_34/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_34/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_35), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_35/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_35), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_35/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_35/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_36), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_36/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_36), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_36/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_36/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_37), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_37/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_37), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_37/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_37/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_38), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_38/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_38), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_38/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_38/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_39), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_39/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_39), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_39/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_39/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_40), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_40/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_40), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_40/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_40/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_41), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_41/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_41), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_41/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_41/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_42), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_42/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_42), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_42/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_42/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_43), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_43/kernel:0' shape=(3, 3, 512, 1024) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_43), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_43/gamma:0' shape=(1024,) dtype=float32>\n","  <tf.Variable 'batch_normalization_43/beta:0' shape=(1024,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_44), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_44/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_44), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_44/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_44/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_45), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_45/kernel:0' shape=(3, 3, 512, 1024) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_45), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_45/gamma:0' shape=(1024,) dtype=float32>\n","  <tf.Variable 'batch_normalization_45/beta:0' shape=(1024,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_46), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_46/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_46), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_46/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_46/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_47), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_47/kernel:0' shape=(3, 3, 512, 1024) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_47), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_47/gamma:0' shape=(1024,) dtype=float32>\n","  <tf.Variable 'batch_normalization_47/beta:0' shape=(1024,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_48), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_48/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_48), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_48/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_48/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_49), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_49/kernel:0' shape=(3, 3, 512, 1024) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_49), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_49/gamma:0' shape=(1024,) dtype=float32>\n","  <tf.Variable 'batch_normalization_49/beta:0' shape=(1024,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_50), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_50/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_50), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_50/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_50/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_51), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_51/kernel:0' shape=(3, 3, 512, 1024) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_51), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_51/gamma:0' shape=(1024,) dtype=float32>\n","  <tf.Variable 'batch_normalization_51/beta:0' shape=(1024,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_52), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_52/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_52), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_52/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_52/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_53), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_53/kernel:0' shape=(3, 3, 512, 1024) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_53), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_53/gamma:0' shape=(1024,) dtype=float32>\n","  <tf.Variable 'batch_normalization_53/beta:0' shape=(1024,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_54), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_54/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_54), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_54/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_54/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_55), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_55/kernel:0' shape=(3, 3, 512, 1024) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_55), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_55/gamma:0' shape=(1024,) dtype=float32>\n","  <tf.Variable 'batch_normalization_55/beta:0' shape=(1024,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_56), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_56/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_56), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_56/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_56/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_57), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_57/kernel:0' shape=(3, 3, 512, 1024) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_57), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_57/gamma:0' shape=(1024,) dtype=float32>\n","  <tf.Variable 'batch_normalization_57/beta:0' shape=(1024,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_58), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_last_layer1_80/kernel:0' shape=(1, 1, 1024, 255) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_last_layer1_80/bias:0' shape=(255,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_59), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_58/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_58), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_58/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_58/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_60), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_59/kernel:0' shape=(1, 1, 768, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_59), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_59/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_59/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_61), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_60/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_60), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_60/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_60/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_62), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_61/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_61), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_61/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_61/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_63), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_62/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_62), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_62/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_62/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_64), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_63/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_63), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_63/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_63/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_65), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_64/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_64), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_64/gamma:0' shape=(512,) dtype=float32>\n","  <tf.Variable 'batch_normalization_64/beta:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_66), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_last_layer2_80/kernel:0' shape=(1, 1, 512, 255) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_last_layer2_80/bias:0' shape=(255,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_67), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_65/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_65), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_65/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_65/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_68), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_66/kernel:0' shape=(1, 1, 384, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_66), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_66/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_66/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_69), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_67/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_67), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_67/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_67/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_70), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_68/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_68), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_68/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_68/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_71), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_69/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_69), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_69/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_69/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_72), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_70/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_70), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_70/gamma:0' shape=(128,) dtype=float32>\n","  <tf.Variable 'batch_normalization_70/beta:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_73), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_71/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_71), but\n","are not present in its tracked objects:\n","  <tf.Variable 'batch_normalization_71/gamma:0' shape=(256,) dtype=float32>\n","  <tf.Variable 'batch_normalization_71/beta:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_74), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_last_layer3_80/kernel:0' shape=(1, 1, 256, 255) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_last_layer3_80/bias:0' shape=(255,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","Model: \"Yolo-V3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n","__________________________________________________________________________________________________\n","tf.nn.convolution (TFOpLambda)  (None, 416, 416, 32) 0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 416, 416, 32 0           tf.nn.convolution[0][0]          \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu (TFOpLambda)   (None, 416, 416, 32) 0           tf.compat.v1.nn.fused_batch_norm[\n","__________________________________________________________________________________________________\n","tf.compat.v1.pad (TFOpLambda)   (None, 417, 417, 32) 0           tf.nn.leaky_relu[0][0]           \n","__________________________________________________________________________________________________\n","tf.nn.convolution_1 (TFOpLambda (None, 208, 208, 64) 0           tf.compat.v1.pad[0][0]           \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 208, 208, 64 0           tf.nn.convolution_1[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_1 (TFOpLambda) (None, 208, 208, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_2 (TFOpLambda (None, 208, 208, 32) 0           tf.nn.leaky_relu_1[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 208, 208, 32 0           tf.nn.convolution_2[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_2 (TFOpLambda) (None, 208, 208, 32) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_3 (TFOpLambda (None, 208, 208, 64) 0           tf.nn.leaky_relu_2[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 208, 208, 64 0           tf.nn.convolution_3[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_3 (TFOpLambda) (None, 208, 208, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add (TFOpLambd (None, 208, 208, 64) 0           tf.nn.leaky_relu_1[0][0]         \n","                                                                 tf.nn.leaky_relu_3[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.pad_1 (TFOpLambda) (None, 209, 209, 64) 0           tf.__operators__.add[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.convolution_4 (TFOpLambda (None, 104, 104, 128 0           tf.compat.v1.pad_1[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 104, 104, 12 0           tf.nn.convolution_4[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_4 (TFOpLambda) (None, 104, 104, 128 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_5 (TFOpLambda (None, 104, 104, 64) 0           tf.nn.leaky_relu_4[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 104, 104, 64 0           tf.nn.convolution_5[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_5 (TFOpLambda) (None, 104, 104, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_6 (TFOpLambda (None, 104, 104, 128 0           tf.nn.leaky_relu_5[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 104, 104, 12 0           tf.nn.convolution_6[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_6 (TFOpLambda) (None, 104, 104, 128 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_1 (TFOpLam (None, 104, 104, 128 0           tf.nn.leaky_relu_4[0][0]         \n","                                                                 tf.nn.leaky_relu_6[0][0]         \n","__________________________________________________________________________________________________\n","tf.nn.convolution_7 (TFOpLambda (None, 104, 104, 64) 0           tf.__operators__.add_1[0][0]     \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 104, 104, 64 0           tf.nn.convolution_7[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_7 (TFOpLambda) (None, 104, 104, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_8 (TFOpLambda (None, 104, 104, 128 0           tf.nn.leaky_relu_7[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 104, 104, 12 0           tf.nn.convolution_8[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_8 (TFOpLambda) (None, 104, 104, 128 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_2 (TFOpLam (None, 104, 104, 128 0           tf.__operators__.add_1[0][0]     \n","                                                                 tf.nn.leaky_relu_8[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.pad_2 (TFOpLambda) (None, 105, 105, 128 0           tf.__operators__.add_2[0][0]     \n","__________________________________________________________________________________________________\n","tf.nn.convolution_9 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.pad_2[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_9[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_9 (TFOpLambda) (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_10 (TFOpLambd (None, 52, 52, 128)  0           tf.nn.leaky_relu_9[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_10[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_10 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_11 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_10[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_11[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_11 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_3 (TFOpLam (None, 52, 52, 256)  0           tf.nn.leaky_relu_9[0][0]         \n","                                                                 tf.nn.leaky_relu_11[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_12 (TFOpLambd (None, 52, 52, 128)  0           tf.__operators__.add_3[0][0]     \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_12[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_12 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_13 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_12[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_13[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_13 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_4 (TFOpLam (None, 52, 52, 256)  0           tf.__operators__.add_3[0][0]     \n","                                                                 tf.nn.leaky_relu_13[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_14 (TFOpLambd (None, 52, 52, 128)  0           tf.__operators__.add_4[0][0]     \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_14[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_14 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_15 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_14[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_15[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_15 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_5 (TFOpLam (None, 52, 52, 256)  0           tf.__operators__.add_4[0][0]     \n","                                                                 tf.nn.leaky_relu_15[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_16 (TFOpLambd (None, 52, 52, 128)  0           tf.__operators__.add_5[0][0]     \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_16[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_16 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_17 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_16[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_17[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_17 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_6 (TFOpLam (None, 52, 52, 256)  0           tf.__operators__.add_5[0][0]     \n","                                                                 tf.nn.leaky_relu_17[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_18 (TFOpLambd (None, 52, 52, 128)  0           tf.__operators__.add_6[0][0]     \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_18[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_18 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_19 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_18[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_19[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_19 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_7 (TFOpLam (None, 52, 52, 256)  0           tf.__operators__.add_6[0][0]     \n","                                                                 tf.nn.leaky_relu_19[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_20 (TFOpLambd (None, 52, 52, 128)  0           tf.__operators__.add_7[0][0]     \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_20[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_20 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_21 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_20[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_21[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_21 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_8 (TFOpLam (None, 52, 52, 256)  0           tf.__operators__.add_7[0][0]     \n","                                                                 tf.nn.leaky_relu_21[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_22 (TFOpLambd (None, 52, 52, 128)  0           tf.__operators__.add_8[0][0]     \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_22[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_22 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_23 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_22[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_23[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_23 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_9 (TFOpLam (None, 52, 52, 256)  0           tf.__operators__.add_8[0][0]     \n","                                                                 tf.nn.leaky_relu_23[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_24 (TFOpLambd (None, 52, 52, 128)  0           tf.__operators__.add_9[0][0]     \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_24[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_24 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_25 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_24[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_25[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_25 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_10 (TFOpLa (None, 52, 52, 256)  0           tf.__operators__.add_9[0][0]     \n","                                                                 tf.nn.leaky_relu_25[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.pad_3 (TFOpLambda) (None, 53, 53, 256)  0           tf.__operators__.add_10[0][0]    \n","__________________________________________________________________________________________________\n","tf.nn.convolution_26 (TFOpLambd (None, 26, 26, 512)  0           tf.compat.v1.pad_3[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_26[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_26 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_27 (TFOpLambd (None, 26, 26, 256)  0           tf.nn.leaky_relu_26[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_27[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_27 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_28 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_27[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_28[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_28 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_11 (TFOpLa (None, 26, 26, 512)  0           tf.nn.leaky_relu_26[0][0]        \n","                                                                 tf.nn.leaky_relu_28[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_29 (TFOpLambd (None, 26, 26, 256)  0           tf.__operators__.add_11[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_29[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_29 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_30 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_29[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_30[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_30 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_12 (TFOpLa (None, 26, 26, 512)  0           tf.__operators__.add_11[0][0]    \n","                                                                 tf.nn.leaky_relu_30[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_31 (TFOpLambd (None, 26, 26, 256)  0           tf.__operators__.add_12[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_31[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_31 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_32 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_31[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_32[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_32 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_13 (TFOpLa (None, 26, 26, 512)  0           tf.__operators__.add_12[0][0]    \n","                                                                 tf.nn.leaky_relu_32[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_33 (TFOpLambd (None, 26, 26, 256)  0           tf.__operators__.add_13[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_33[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_33 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_34 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_33[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_34[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_34 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_14 (TFOpLa (None, 26, 26, 512)  0           tf.__operators__.add_13[0][0]    \n","                                                                 tf.nn.leaky_relu_34[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_35 (TFOpLambd (None, 26, 26, 256)  0           tf.__operators__.add_14[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_35[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_35 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_36 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_35[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_36[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_36 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_15 (TFOpLa (None, 26, 26, 512)  0           tf.__operators__.add_14[0][0]    \n","                                                                 tf.nn.leaky_relu_36[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_37 (TFOpLambd (None, 26, 26, 256)  0           tf.__operators__.add_15[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_37[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_37 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_38 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_37[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_38[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_38 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_16 (TFOpLa (None, 26, 26, 512)  0           tf.__operators__.add_15[0][0]    \n","                                                                 tf.nn.leaky_relu_38[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_39 (TFOpLambd (None, 26, 26, 256)  0           tf.__operators__.add_16[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_39[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_39 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_40 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_39[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_40[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_40 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_17 (TFOpLa (None, 26, 26, 512)  0           tf.__operators__.add_16[0][0]    \n","                                                                 tf.nn.leaky_relu_40[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_41 (TFOpLambd (None, 26, 26, 256)  0           tf.__operators__.add_17[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_41[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_41 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_42 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_41[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_42[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_42 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_18 (TFOpLa (None, 26, 26, 512)  0           tf.__operators__.add_17[0][0]    \n","                                                                 tf.nn.leaky_relu_42[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.pad_4 (TFOpLambda) (None, 27, 27, 512)  0           tf.__operators__.add_18[0][0]    \n","__________________________________________________________________________________________________\n","tf.nn.convolution_43 (TFOpLambd (None, 13, 13, 1024) 0           tf.compat.v1.pad_4[0][0]         \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 1024 0           tf.nn.convolution_43[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_43 (TFOpLambda (None, 13, 13, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_44 (TFOpLambd (None, 13, 13, 512)  0           tf.nn.leaky_relu_43[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 512) 0           tf.nn.convolution_44[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_44 (TFOpLambda (None, 13, 13, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_45 (TFOpLambd (None, 13, 13, 1024) 0           tf.nn.leaky_relu_44[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 1024 0           tf.nn.convolution_45[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_45 (TFOpLambda (None, 13, 13, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_19 (TFOpLa (None, 13, 13, 1024) 0           tf.nn.leaky_relu_43[0][0]        \n","                                                                 tf.nn.leaky_relu_45[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_46 (TFOpLambd (None, 13, 13, 512)  0           tf.__operators__.add_19[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 512) 0           tf.nn.convolution_46[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_46 (TFOpLambda (None, 13, 13, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_47 (TFOpLambd (None, 13, 13, 1024) 0           tf.nn.leaky_relu_46[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 1024 0           tf.nn.convolution_47[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_47 (TFOpLambda (None, 13, 13, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_20 (TFOpLa (None, 13, 13, 1024) 0           tf.__operators__.add_19[0][0]    \n","                                                                 tf.nn.leaky_relu_47[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_48 (TFOpLambd (None, 13, 13, 512)  0           tf.__operators__.add_20[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 512) 0           tf.nn.convolution_48[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_48 (TFOpLambda (None, 13, 13, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_49 (TFOpLambd (None, 13, 13, 1024) 0           tf.nn.leaky_relu_48[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 1024 0           tf.nn.convolution_49[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_49 (TFOpLambda (None, 13, 13, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_21 (TFOpLa (None, 13, 13, 1024) 0           tf.__operators__.add_20[0][0]    \n","                                                                 tf.nn.leaky_relu_49[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_50 (TFOpLambd (None, 13, 13, 512)  0           tf.__operators__.add_21[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 512) 0           tf.nn.convolution_50[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_50 (TFOpLambda (None, 13, 13, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_51 (TFOpLambd (None, 13, 13, 1024) 0           tf.nn.leaky_relu_50[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 1024 0           tf.nn.convolution_51[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_51 (TFOpLambda (None, 13, 13, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.__operators__.add_22 (TFOpLa (None, 13, 13, 1024) 0           tf.__operators__.add_21[0][0]    \n","                                                                 tf.nn.leaky_relu_51[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_52 (TFOpLambd (None, 13, 13, 512)  0           tf.__operators__.add_22[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 512) 0           tf.nn.convolution_52[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_52 (TFOpLambda (None, 13, 13, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_53 (TFOpLambd (None, 13, 13, 1024) 0           tf.nn.leaky_relu_52[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 1024 0           tf.nn.convolution_53[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_53 (TFOpLambda (None, 13, 13, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_54 (TFOpLambd (None, 13, 13, 512)  0           tf.nn.leaky_relu_53[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 512) 0           tf.nn.convolution_54[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_54 (TFOpLambda (None, 13, 13, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_55 (TFOpLambd (None, 13, 13, 1024) 0           tf.nn.leaky_relu_54[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 1024 0           tf.nn.convolution_55[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_55 (TFOpLambda (None, 13, 13, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_56 (TFOpLambd (None, 13, 13, 512)  0           tf.nn.leaky_relu_55[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 512) 0           tf.nn.convolution_56[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_56 (TFOpLambda (None, 13, 13, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_59 (TFOpLambd (None, 13, 13, 256)  0           tf.nn.leaky_relu_56[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 256) 0           tf.nn.convolution_59[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_58 (TFOpLambda (None, 13, 13, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","up_sampling2d (UpSampling2D)    (None, 26, 26, 256)  0           tf.nn.leaky_relu_58[0][0]        \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 26, 26, 768)  0           up_sampling2d[0][0]              \n","                                                                 tf.__operators__.add_18[0][0]    \n","__________________________________________________________________________________________________\n","tf.nn.convolution_60 (TFOpLambd (None, 26, 26, 256)  0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_60[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_59 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_61 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_59[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_61[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_60 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_62 (TFOpLambd (None, 26, 26, 256)  0           tf.nn.leaky_relu_60[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_62[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_61 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_63 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_61[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_63[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_62 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_64 (TFOpLambd (None, 26, 26, 256)  0           tf.nn.leaky_relu_62[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 256) 0           tf.nn.convolution_64[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_63 (TFOpLambda (None, 26, 26, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_67 (TFOpLambd (None, 26, 26, 128)  0           tf.nn.leaky_relu_63[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 128) 0           tf.nn.convolution_67[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_65 (TFOpLambda (None, 26, 26, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 52, 52, 128)  0           tf.nn.leaky_relu_65[0][0]        \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 52, 52, 384)  0           up_sampling2d_1[0][0]            \n","                                                                 tf.__operators__.add_10[0][0]    \n","__________________________________________________________________________________________________\n","tf.nn.convolution_68 (TFOpLambd (None, 52, 52, 128)  0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_68[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_66 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_69 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_66[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_69[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_67 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_70 (TFOpLambd (None, 52, 52, 128)  0           tf.nn.leaky_relu_67[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_70[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_68 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_71 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_68[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_71[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_69 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_72 (TFOpLambd (None, 52, 52, 128)  0           tf.nn.leaky_relu_69[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 128) 0           tf.nn.convolution_72[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_70 (TFOpLambda (None, 52, 52, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_57 (TFOpLambd (None, 13, 13, 1024) 0           tf.nn.leaky_relu_56[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_65 (TFOpLambd (None, 26, 26, 512)  0           tf.nn.leaky_relu_63[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_73 (TFOpLambd (None, 52, 52, 256)  0           tf.nn.leaky_relu_70[0][0]        \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 13, 13, 1024 0           tf.nn.convolution_57[0][0]       \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 26, 26, 512) 0           tf.nn.convolution_65[0][0]       \n","__________________________________________________________________________________________________\n","tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 256) 0           tf.nn.convolution_73[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_57 (TFOpLambda (None, 13, 13, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_64 (TFOpLambda (None, 26, 26, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.leaky_relu_71 (TFOpLambda (None, 52, 52, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n","__________________________________________________________________________________________________\n","tf.nn.convolution_58 (TFOpLambd (None, 13, 13, 255)  0           tf.nn.leaky_relu_57[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_66 (TFOpLambd (None, 26, 26, 255)  0           tf.nn.leaky_relu_64[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.convolution_74 (TFOpLambd (None, 52, 52, 255)  0           tf.nn.leaky_relu_71[0][0]        \n","__________________________________________________________________________________________________\n","tf.nn.bias_add (TFOpLambda)     (None, 13, 13, 255)  0           tf.nn.convolution_58[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.bias_add_1 (TFOpLambda)   (None, 26, 26, 255)  0           tf.nn.convolution_66[0][0]       \n","__________________________________________________________________________________________________\n","tf.nn.bias_add_2 (TFOpLambda)   (None, 52, 52, 255)  0           tf.nn.convolution_74[0][0]       \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 13, 13, 3, 85 0           tf.nn.bias_add[0][0]             \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 26, 26, 3, 85 0           tf.nn.bias_add_1[0][0]           \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 52, 52, 3, 85 0           tf.nn.bias_add_2[0][0]           \n","__________________________________________________________________________________________________\n","tf.split (TFOpLambda)           [(None, 13, 13, 3, 2 0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","tf.split_1 (TFOpLambda)         [(None, 26, 26, 3, 2 0           tf.reshape_1[0][0]               \n","__________________________________________________________________________________________________\n","tf.split_2 (TFOpLambda)         [(None, 52, 52, 3, 2 0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.math.exp (TFOpLambda)        (None, 13, 13, 3, 2) 0           tf.split[0][1]                   \n","__________________________________________________________________________________________________\n","tf.math.exp_1 (TFOpLambda)      (None, 26, 26, 3, 2) 0           tf.split_1[0][1]                 \n","__________________________________________________________________________________________________\n","tf.math.exp_2 (TFOpLambda)      (None, 52, 52, 3, 2) 0           tf.split_2[0][1]                 \n","__________________________________________________________________________________________________\n","tf.math.sigmoid (TFOpLambda)    (None, 13, 13, 3, 2) 0           tf.split[0][0]                   \n","__________________________________________________________________________________________________\n","tf.math.multiply (TFOpLambda)   (None, 13, 13, 3, 2) 0           tf.math.exp[0][0]                \n","__________________________________________________________________________________________________\n","tf.math.sigmoid_3 (TFOpLambda)  (None, 26, 26, 3, 2) 0           tf.split_1[0][0]                 \n","__________________________________________________________________________________________________\n","tf.math.multiply_1 (TFOpLambda) (None, 26, 26, 3, 2) 0           tf.math.exp_1[0][0]              \n","__________________________________________________________________________________________________\n","tf.math.sigmoid_6 (TFOpLambda)  (None, 52, 52, 3, 2) 0           tf.split_2[0][0]                 \n","__________________________________________________________________________________________________\n","tf.math.multiply_2 (TFOpLambda) (None, 52, 52, 3, 2) 0           tf.math.exp_2[0][0]              \n","__________________________________________________________________________________________________\n","tf.__operators__.add_23 (TFOpLa (None, 13, 13, 3, 2) 0           tf.math.sigmoid[0][0]            \n","__________________________________________________________________________________________________\n","tf.math.truediv_1 (TFOpLambda)  (None, 13, 13, 3, 2) 0           tf.math.multiply[0][0]           \n","__________________________________________________________________________________________________\n","tf.__operators__.add_25 (TFOpLa (None, 26, 26, 3, 2) 0           tf.math.sigmoid_3[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.truediv_5 (TFOpLambda)  (None, 26, 26, 3, 2) 0           tf.math.multiply_1[0][0]         \n","__________________________________________________________________________________________________\n","tf.__operators__.add_27 (TFOpLa (None, 52, 52, 3, 2) 0           tf.math.sigmoid_6[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.truediv_9 (TFOpLambda)  (None, 52, 52, 3, 2) 0           tf.math.multiply_2[0][0]         \n","__________________________________________________________________________________________________\n","tf.math.truediv (TFOpLambda)    (None, 13, 13, 3, 2) 0           tf.__operators__.add_23[0][0]    \n","__________________________________________________________________________________________________\n","tf.math.truediv_2 (TFOpLambda)  (None, 13, 13, 3, 2) 0           tf.math.truediv_1[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.truediv_3 (TFOpLambda)  (None, 13, 13, 3, 2) 0           tf.math.truediv_1[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.truediv_4 (TFOpLambda)  (None, 26, 26, 3, 2) 0           tf.__operators__.add_25[0][0]    \n","__________________________________________________________________________________________________\n","tf.math.truediv_6 (TFOpLambda)  (None, 26, 26, 3, 2) 0           tf.math.truediv_5[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.truediv_7 (TFOpLambda)  (None, 26, 26, 3, 2) 0           tf.math.truediv_5[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.truediv_8 (TFOpLambda)  (None, 52, 52, 3, 2) 0           tf.__operators__.add_27[0][0]    \n","__________________________________________________________________________________________________\n","tf.math.truediv_10 (TFOpLambda) (None, 52, 52, 3, 2) 0           tf.math.truediv_9[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.truediv_11 (TFOpLambda) (None, 52, 52, 3, 2) 0           tf.math.truediv_9[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.subtract (TFOpLambda)   (None, 13, 13, 3, 2) 0           tf.math.truediv[0][0]            \n","                                                                 tf.math.truediv_2[0][0]          \n","__________________________________________________________________________________________________\n","tf.__operators__.add_24 (TFOpLa (None, 13, 13, 3, 2) 0           tf.math.truediv[0][0]            \n","                                                                 tf.math.truediv_3[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.subtract_1 (TFOpLambda) (None, 26, 26, 3, 2) 0           tf.math.truediv_4[0][0]          \n","                                                                 tf.math.truediv_6[0][0]          \n","__________________________________________________________________________________________________\n","tf.__operators__.add_26 (TFOpLa (None, 26, 26, 3, 2) 0           tf.math.truediv_4[0][0]          \n","                                                                 tf.math.truediv_7[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.subtract_2 (TFOpLambda) (None, 52, 52, 3, 2) 0           tf.math.truediv_8[0][0]          \n","                                                                 tf.math.truediv_10[0][0]         \n","__________________________________________________________________________________________________\n","tf.__operators__.add_28 (TFOpLa (None, 52, 52, 3, 2) 0           tf.math.truediv_8[0][0]          \n","                                                                 tf.math.truediv_11[0][0]         \n","__________________________________________________________________________________________________\n","tf.math.sigmoid_1 (TFOpLambda)  (None, 13, 13, 3, 1) 0           tf.split[0][2]                   \n","__________________________________________________________________________________________________\n","tf.math.sigmoid_4 (TFOpLambda)  (None, 26, 26, 3, 1) 0           tf.split_1[0][2]                 \n","__________________________________________________________________________________________________\n","tf.math.sigmoid_7 (TFOpLambda)  (None, 52, 52, 3, 1) 0           tf.split_2[0][2]                 \n","__________________________________________________________________________________________________\n","tf.math.sigmoid_2 (TFOpLambda)  (None, 13, 13, 3, 80 0           tf.split[0][3]                   \n","__________________________________________________________________________________________________\n","tf.math.sigmoid_5 (TFOpLambda)  (None, 26, 26, 3, 80 0           tf.split_1[0][3]                 \n","__________________________________________________________________________________________________\n","tf.math.sigmoid_8 (TFOpLambda)  (None, 52, 52, 3, 80 0           tf.split_2[0][3]                 \n","__________________________________________________________________________________________________\n","tf.concat_1 (TFOpLambda)        (None, 13, 13, 3, 4) 0           tf.math.subtract[0][0]           \n","                                                                 tf.__operators__.add_24[0][0]    \n","__________________________________________________________________________________________________\n","tf.concat_3 (TFOpLambda)        (None, 26, 26, 3, 4) 0           tf.math.subtract_1[0][0]         \n","                                                                 tf.__operators__.add_26[0][0]    \n","__________________________________________________________________________________________________\n","tf.concat_5 (TFOpLambda)        (None, 52, 52, 3, 4) 0           tf.math.subtract_2[0][0]         \n","                                                                 tf.__operators__.add_28[0][0]    \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_1 (TFOpLambd (5,)                 0           tf.math.sigmoid_1[0][0]          \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_4 (TFOpLambd (5,)                 0           tf.math.sigmoid_4[0][0]          \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_7 (TFOpLambd (5,)                 0           tf.math.sigmoid_7[0][0]          \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_2 (TFOpLambd (5,)                 0           tf.math.sigmoid_2[0][0]          \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_5 (TFOpLambd (5,)                 0           tf.math.sigmoid_5[0][0]          \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_8 (TFOpLambd (5,)                 0           tf.math.sigmoid_8[0][0]          \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape (TFOpLambda) (5,)                 0           tf.concat_1[0][0]                \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_3 (TFOpLambd (5,)                 0           tf.concat_3[0][0]                \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_6 (TFOpLambd (5,)                 0           tf.concat_5[0][0]                \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_1 (Sli ()                   0           tf.compat.v1.shape_1[0][0]       \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_4 (Sli ()                   0           tf.compat.v1.shape_4[0][0]       \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_7 (Sli ()                   0           tf.compat.v1.shape_7[0][0]       \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_2 (Sli ()                   0           tf.compat.v1.shape_2[0][0]       \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_5 (Sli ()                   0           tf.compat.v1.shape_5[0][0]       \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_8 (Sli ()                   0           tf.compat.v1.shape_8[0][0]       \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem (Slici ()                   0           tf.compat.v1.shape[0][0]         \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_3 (Sli ()                   0           tf.compat.v1.shape_3[0][0]       \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_6 (Sli ()                   0           tf.compat.v1.shape_6[0][0]       \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, None, 1)      0           tf.math.sigmoid_1[0][0]          \n","                                                                 tf.__operators__.getitem_1[0][0] \n","__________________________________________________________________________________________________\n","tf.reshape_7 (TFOpLambda)       (None, None, 1)      0           tf.math.sigmoid_4[0][0]          \n","                                                                 tf.__operators__.getitem_4[0][0] \n","__________________________________________________________________________________________________\n","tf.reshape_10 (TFOpLambda)      (None, None, 1)      0           tf.math.sigmoid_7[0][0]          \n","                                                                 tf.__operators__.getitem_7[0][0] \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, None, 80)     0           tf.math.sigmoid_2[0][0]          \n","                                                                 tf.__operators__.getitem_2[0][0] \n","__________________________________________________________________________________________________\n","tf.reshape_8 (TFOpLambda)       (None, None, 80)     0           tf.math.sigmoid_5[0][0]          \n","                                                                 tf.__operators__.getitem_5[0][0] \n","__________________________________________________________________________________________________\n","tf.reshape_11 (TFOpLambda)      (None, None, 80)     0           tf.math.sigmoid_8[0][0]          \n","                                                                 tf.__operators__.getitem_8[0][0] \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, None, 4)      0           tf.concat_1[0][0]                \n","                                                                 tf.__operators__.getitem[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_6 (TFOpLambda)       (None, None, 4)      0           tf.concat_3[0][0]                \n","                                                                 tf.__operators__.getitem_3[0][0] \n","__________________________________________________________________________________________________\n","tf.reshape_9 (TFOpLambda)       (None, None, 4)      0           tf.concat_5[0][0]                \n","                                                                 tf.__operators__.getitem_6[0][0] \n","__________________________________________________________________________________________________\n","tf.concat_7 (TFOpLambda)        (None, None, 1)      0           tf.reshape_4[0][0]               \n","                                                                 tf.reshape_7[0][0]               \n","                                                                 tf.reshape_10[0][0]              \n","__________________________________________________________________________________________________\n","tf.concat_8 (TFOpLambda)        (None, None, 80)     0           tf.reshape_5[0][0]               \n","                                                                 tf.reshape_8[0][0]               \n","                                                                 tf.reshape_11[0][0]              \n","__________________________________________________________________________________________________\n","tf.concat_6 (TFOpLambda)        (None, None, 4)      0           tf.reshape_3[0][0]               \n","                                                                 tf.reshape_6[0][0]               \n","                                                                 tf.reshape_9[0][0]               \n","__________________________________________________________________________________________________\n","tf.math.multiply_3 (TFOpLambda) (None, None, 80)     0           tf.concat_7[0][0]                \n","                                                                 tf.concat_8[0][0]                \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_9 (TFOpLambd (3,)                 0           tf.concat_6[0][0]                \n","__________________________________________________________________________________________________\n","tf.compat.v1.shape_10 (TFOpLamb (3,)                 0           tf.math.multiply_3[0][0]         \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_9 (Sli ()                   0           tf.compat.v1.shape_9[0][0]       \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem_10 (Sl ()                   0           tf.compat.v1.shape_10[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape_12 (TFOpLambda)      (None, None, 1, 4)   0           tf.concat_6[0][0]                \n","                                                                 tf.__operators__.getitem_9[0][0] \n","__________________________________________________________________________________________________\n","tf.reshape_13 (TFOpLambda)      (None, None, 80)     0           tf.math.multiply_3[0][0]         \n","                                                                 tf.__operators__.getitem_10[0][0]\n","__________________________________________________________________________________________________\n","tf.image.combined_non_max_suppr CombinedNonMaxSuppre 0           tf.reshape_12[0][0]              \n","                                                                 tf.reshape_13[0][0]              \n","==================================================================================================\n","Total params: 0\n","Trainable params: 0\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","model created\n","Traceback (most recent call last):\n","  File \"convert.py\", line 35, in <module>\n","    main()\n","  File \"convert.py\", line 23, in main\n","    load_darknet_weights(yolo, yolo_darknet_weights)\n","  File \"/content/TF2-Yolo3/utils/utils.py\", line 16, in load_darknet_weights\n","    layers_list.append(model.layers[1].get_layer(conv_name))\n","AttributeError: 'TFOpLambda' object has no attribute 'get_layer'\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"id":"-mBZzNC6l_Xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python test.py"],"metadata":{"id":"b1sLPlD9ASOI"},"execution_count":null,"outputs":[]}]}